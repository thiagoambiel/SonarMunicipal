{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13736556,"sourceType":"datasetVersion","datasetId":8740224}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate sacrebleu bert_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os, json, math, random, re, html\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\n\nimport datasets\nfrom datasets import load_dataset, DatasetDict\nimport evaluate\nimport numpy as np\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    set_seed,\n)\n\n# PEFT/LoRA opcionais\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\ntorch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:34:11.023392Z","iopub.execute_input":"2025-11-14T22:34:11.023662Z","iopub.status.idle":"2025-11-14T22:34:21.347003Z","shell.execute_reply.started":"2025-11-14T22:34:11.023639Z","shell.execute_reply":"2025-11-14T22:34:21.346199Z"}},"outputs":[{"name":"stderr","text":"2025-11-14 22:34:15.508038: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763159655.531032     577 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763159655.538280     577 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"(True, 2, 'Tesla T4')"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# ===== CONFIGURAÇÕES EDITÁVEIS =====\nPROJECT_NAME    = \"ptt5v2-pl-text2action\"\nDATA_PATH       = \"/kaggle/input/pl-to-action-dataset/pl_action_recommendations_all.jsonl\"\nOUTPUT_DIR      = \"./outputs_ptt5v2\"              # onde salvar checkpoints\nEVAL_SPLIT      = 0.1                              # fração para validação\nMAX_INPUT_LEN   = 256\nMAX_TARGET_LEN  = 32\n\n# Escolha do modelo base (PTT5-v2). Alguns checkpoints comuns:\n# - \"unicamp-dl/ptt5-base-portuguese-vocab\"  (mais conhecido)\n# - \"pierreguillou/ptt5-base-portuguese-vocab\" (espelho)\n# Se você já tem um \"PTT5-v2\" específico, coloque o ID abaixo.\nMODEL_ID        = \"unicamp-dl/ptt5-base-portuguese-vocab\"\n\n# Hiperparâmetros sugeridos (ajuste conforme sua GPU)\nSEED            = 42\nBATCH_SIZE      = 16\nGRAD_ACC_STEPS  = 2\nLR              = 3e-4\nEPOCHS          = 30\nFP16            = torch.cuda.is_available()\nBF16            = False   # Ative se a sua GPU suportar (A100/A800/H100/RTX 5xxx)\nWARMUP_RATIO    = 0.03\nWEIGHT_DECAY    = 0.01\n\n# QLoRA (8-bit/4-bit) opções\nUSE_4BIT        = True    # True = quantização 4-bit (menos VRAM); False = 8-bit\nLORA_R          = 16\nLORA_ALPHA      = 32\nLORA_DROPOUT    = 0.05\n\n# Template simples de instrução (pode adaptar)\nINSTR_PROMPT = (\n  \"Converta a ementa de projeto de lei em uma recomendação de ação imperativa, curta e fiel ao texto; \"\n  \"{texto}\\nSaída:\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:34:21.348289Z","iopub.execute_input":"2025-11-14T22:34:21.348865Z","iopub.status.idle":"2025-11-14T22:34:21.354418Z","shell.execute_reply.started":"2025-11-14T22:34:21.348845Z","shell.execute_reply":"2025-11-14T22:34:21.353572Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"INSTR_PROMPT = (\n    \"Dada a ementa de um projeto de lei em linguagem jurídica, gere uma única recomendação de ação operacional em português no formato [Verbo no infinitivo] + [objeto] + [complementos essenciais], por exemplo: “Implantar estufas com hortas produzidas com garrafas PET nas escolas municipais.” Remova toda a “casca jurídica” que não muda a ação (“Dispõe sobre…”, “Institui…”, “Cria…”, “Autoriza o Poder Executivo a…”, “e dá outras providências”, referências a leis, artigos e fórmulas padrão), preservando apenas o conteúdo material da política: o que passa a existir, ser feito, fornecido ou garantido. Identifique o núcleo da ementa (substantivos de ação como criação, implantação, emissão, fornecimento, atendimento etc.) e transforme-o em verbo no infinitivo impessoal (criar, implantar, emitir, fornecer, atender etc.), seguido do objeto principal e dos complementos realmente necessários (público-alvo e/ou local, quando essenciais para entender a execução). Quando a ementa criar um equipamento, serviço, órgão ou programa (inclusive digitais), a ação deve ser “criar” ou “implantar” esse instrumento; quando houver estrutura do tipo “Programa/Projeto X para [substantivo de ação]…”, priorize o serviço final (ex.: “emissão de registro de nascimento” → “Emitir registros de nascimento dentro das maternidades públicas”) e não o programa em si. Neutralize nomes fantasia de programas (“Segurinho”, “Saúde ao Alcance” etc.), descrevendo-os de forma genérica pelo tipo de programa/serviço, a menos de datas comemorativas, prêmios, selos ou eventos culturais, em que o nome é o próprio objeto e deve ser mantido. Elimine justificativas, fundamentos legais e detalhes que não alteram a execução, produzindo sempre uma frase imperativa, curta e operacional, algo que caiba em um backlog de políticas públicas.\"\n    \"\\n\\nEmenta: {texto}\\n\\nAção: \"\n)\n\nprint(INSTR_PROMPT)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize_text(s: str) -> str:\n    # Remove entidades HTML, que aparecem com frequência no seu exemplo (&#8211;, &#8220; etc.)\n    s = html.unescape(s)\n    # Quebras de linha e espaços\n    s = s.replace(\"\\r\", \" \").replace(\"\\n\", \" \").strip()\n    # Espacos múltiplos\n    s = re.sub(r\"\\s+\", \" \", s)\n    return s\n\ndata = pd.read_csv(\"/kaggle/input/ementa2action-dataset/ementa2action_gpt.csv\")\ndata = data.iloc[:, 1:]\ndata = data.rename(columns={\"ementa\": \"input\", \"acao\": \"output\"})\nraw = list(data.T.to_dict().values())\n\nfor r in raw:\n    r[\"input\"]  = normalize_text(r[\"input\"].lower())\n    r[\"output\"] = normalize_text(r[\"output\"])\n\nrandom.seed(SEED)\nrandom.shuffle(raw)\n\nn = len(raw)\nval_n = max(1, int(n * EVAL_SPLIT))\nval_data = raw[:val_n]\ntrain_data = raw[val_n:]\n\ndataset = DatasetDict({\n    \"train\": datasets.Dataset.from_list(train_data),\n    \"validation\": datasets.Dataset.from_list(val_data)\n})\n\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:34:21.355340Z","iopub.execute_input":"2025-11-14T22:34:21.355885Z","iopub.status.idle":"2025-11-14T22:34:21.429493Z","shell.execute_reply.started":"2025-11-14T22:34:21.355865Z","shell.execute_reply":"2025-11-14T22:34:21.428862Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input', 'output'],\n        num_rows: 752\n    })\n    validation: Dataset({\n        features: ['input', 'output'],\n        num_rows: 83\n    })\n})"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n\ndef build_source(texto: str) -> str:\n    return INSTR_PROMPT.format(texto=texto)\n\ndef preprocess_batch(batch):\n    # batch[\"input\"] é uma lista de strings\n    sources = [build_source(t) for t in batch[\"input\"]]\n    targets = batch[\"output\"]\n\n    model_inputs = tokenizer(\n        sources,\n        max_length=MAX_INPUT_LEN,\n        truncation=True,\n        padding=False,\n    )\n    # use o argumento oficial para targets\n    labels = tokenizer(\n        text_target=targets,\n        max_length=MAX_TARGET_LEN,\n        truncation=True,\n        padding=False,\n    )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized = dataset.map(\n    preprocess_batch,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)\n\ntokenized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:34:21.430960Z","iopub.execute_input":"2025-11-14T22:34:21.431189Z","iopub.status.idle":"2025-11-14T22:34:22.196440Z","shell.execute_reply.started":"2025-11-14T22:34:21.431170Z","shell.execute_reply":"2025-11-14T22:34:22.195850Z"}},"outputs":[{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/752 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"523a9f898308470081f03ba35604b198"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/83 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24cfd3499be34f1696925354f64afee1"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 752\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 83\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"bertscore = evaluate.load(\"bertscore\")\n\ndef postprocess_text(preds, labels):\n    preds = [p.strip() for p in preds]\n    labels = [[l.strip()] for l in labels]\n    return preds, labels\n\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n\n    # Substituir -100 por pad_token_id para decodificação\n    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    # BERTScore entre saída do modelo (predictions) e texto alvo (references)\n    bert_result = bertscore.compute(\n        predictions=decoded_preds,\n        references=[l[0] for l in decoded_labels],\n        lang=\"pt\",          # importante para português\n        rescale_with_baseline=True\n    )\n\n    precision = float(np.mean(bert_result[\"precision\"]))\n    recall    = float(np.mean(bert_result[\"recall\"]))\n    f1        = float(np.mean(bert_result[\"f1\"]))\n\n    return {\n        \"bertscore_precision\": round(precision, 4),\n        \"bertscore_recall\": round(recall, 4),\n        \"bertscore_f1\": round(f1, 4),   # normalmente essa é a principal\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:34:22.197098Z","iopub.execute_input":"2025-11-14T22:34:22.197283Z","iopub.status.idle":"2025-11-14T22:34:22.677105Z","shell.execute_reply.started":"2025-11-14T22:34:22.197267Z","shell.execute_reply":"2025-11-14T22:34:22.676322Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_lora_model():\n    # Quantização 4-bit/8-bit (QLoRA)\n    kwargs = dict(\n        device_map=\"auto\",\n        load_in_4bit=USE_4BIT,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16 if BF16 else torch.float16,\n    ) if torch.cuda.is_available() else {}\n\n    base = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID, **kwargs)\n    base = prepare_model_for_kbit_training(base)\n\n    lora_cfg = LoraConfig(\n        r=LORA_R,\n        lora_alpha=LORA_ALPHA,\n        lora_dropout=LORA_DROPOUT,\n        target_modules=[\"q\", \"v\", \"k\", \"o\", \"wi\", \"wo\"],  # nomes comuns em T5\n        bias=\"none\",\n        task_type=\"SEQ_2_SEQ_LM\",\n    )\n    peft_model = get_peft_model(base, lora_cfg)\n    peft_model.print_trainable_parameters()\n    return peft_model\n\nmodel = load_lora_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:34:22.677951Z","iopub.execute_input":"2025-11-14T22:34:22.678233Z","iopub.status.idle":"2025-11-14T22:34:27.378459Z","shell.execute_reply.started":"2025-11-14T22:34:22.678205Z","shell.execute_reply":"2025-11-14T22:34:27.377506Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py:1614: UserWarning: The following device_map keys do not match any submodules in the model: ['decoder.embed_tokens', 'encoder.embed_tokens']\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 6,488,064 || all params: 229,391,616 || trainable%: 2.8284\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    logging_steps=50,\n    save_steps=50,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_bertscore_f1\",\n    greater_is_better=True,\n\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRAD_ACC_STEPS,\n    learning_rate=LR,\n    num_train_epochs=EPOCHS,\n    weight_decay=WEIGHT_DECAY,\n    warmup_ratio=WARMUP_RATIO,\n    lr_scheduler_type=\"cosine\",\n    gradient_checkpointing=True,\n\n    fp16=FP16 and not BF16,\n    bf16=BF16,\n\n    predict_with_generate=True,\n    generation_max_length=MAX_TARGET_LEN,\n    generation_num_beams=4,\n\n    seed=SEED,\n    report_to=[\"none\"],  # mude para [\"tensorboard\"] se quiser\n)\nset_seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:34:27.380282Z","iopub.execute_input":"2025-11-14T22:34:27.380515Z","iopub.status.idle":"2025-11-14T22:34:27.420296Z","shell.execute_reply.started":"2025-11-14T22:34:27.380497Z","shell.execute_reply":"2025-11-14T22:34:27.419481Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\ntrain_result = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:34:27.421039Z","iopub.execute_input":"2025-11-14T22:34:27.421282Z","iopub.status.idle":"2025-11-14T22:51:15.161199Z","shell.execute_reply.started":"2025-11-14T22:34:27.421256Z","shell.execute_reply":"2025-11-14T22:51:15.160462Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_577/310220481.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nNo label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [720/720 16:45, Epoch 30/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bertscore Precision</th>\n      <th>Bertscore Recall</th>\n      <th>Bertscore F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>5.709300</td>\n      <td>1.226335</td>\n      <td>0.621300</td>\n      <td>0.636800</td>\n      <td>0.628400</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.167800</td>\n      <td>0.775525</td>\n      <td>0.752500</td>\n      <td>0.731900</td>\n      <td>0.741800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.858400</td>\n      <td>0.637874</td>\n      <td>0.791800</td>\n      <td>0.751500</td>\n      <td>0.770700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.724200</td>\n      <td>0.576302</td>\n      <td>0.814700</td>\n      <td>0.777700</td>\n      <td>0.795300</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.615400</td>\n      <td>0.533193</td>\n      <td>0.829400</td>\n      <td>0.802700</td>\n      <td>0.815300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.553200</td>\n      <td>0.501034</td>\n      <td>0.836300</td>\n      <td>0.821100</td>\n      <td>0.828400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.491900</td>\n      <td>0.500221</td>\n      <td>0.851600</td>\n      <td>0.823800</td>\n      <td>0.836900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.454500</td>\n      <td>0.490571</td>\n      <td>0.864100</td>\n      <td>0.835500</td>\n      <td>0.849000</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.420600</td>\n      <td>0.478745</td>\n      <td>0.847900</td>\n      <td>0.836300</td>\n      <td>0.841900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.410600</td>\n      <td>0.475219</td>\n      <td>0.849100</td>\n      <td>0.832600</td>\n      <td>0.840600</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.386600</td>\n      <td>0.478659</td>\n      <td>0.852100</td>\n      <td>0.843700</td>\n      <td>0.847600</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.372500</td>\n      <td>0.475411</td>\n      <td>0.850400</td>\n      <td>0.841400</td>\n      <td>0.845700</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.370900</td>\n      <td>0.476113</td>\n      <td>0.848600</td>\n      <td>0.836500</td>\n      <td>0.842300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.369600</td>\n      <td>0.475912</td>\n      <td>0.851400</td>\n      <td>0.841500</td>\n      <td>0.846200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"metrics = train_result.metrics\nmetrics[\"train_samples\"] = len(tokenized[\"train\"])\n\ntrainer.log_metrics(\"train\", metrics)\ntrainer.save_metrics(\"train\", metrics)\ntrainer.save_state()\n\n# Avaliar no validation\neval_metrics = trainer.evaluate()\neval_metrics[\"eval_samples\"] = len(tokenized[\"validation\"])\ntrainer.log_metrics(\"eval\", eval_metrics)\ntrainer.save_metrics(\"eval\", eval_metrics)\n\n# Salvar (se LoRA, salva adaptadores; se FT, salva modelo completo)\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:51:15.162079Z","iopub.execute_input":"2025-11-14T22:51:15.162356Z","iopub.status.idle":"2025-11-14T22:51:31.258072Z","shell.execute_reply.started":"2025-11-14T22:51:15.162337Z","shell.execute_reply":"2025-11-14T22:51:31.257231Z"}},"outputs":[{"name":"stdout","text":"***** train metrics *****\n  epoch                    =       30.0\n  total_flos               =  2419521GF\n  train_loss               =     0.9067\n  train_runtime            = 0:16:47.26\n  train_samples            =        752\n  train_samples_per_second =     22.397\n  train_steps_per_second   =      0.715\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6/6 00:12]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"***** eval metrics *****\n  epoch                    =       30.0\n  eval_bertscore_f1        =      0.849\n  eval_bertscore_precision =     0.8641\n  eval_bertscore_recall    =     0.8355\n  eval_loss                =     0.4906\n  eval_runtime             = 0:00:15.81\n  eval_samples             =         83\n  eval_samples_per_second  =      5.248\n  eval_steps_per_second    =      0.379\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"('./outputs_ptt5v2/tokenizer_config.json',\n './outputs_ptt5v2/special_tokens_map.json',\n './outputs_ptt5v2/spiece.model',\n './outputs_ptt5v2/added_tokens.json',\n './outputs_ptt5v2/tokenizer.json')"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def predict(texto: str, max_new_tokens=64, num_beams=4):\n    inp = INSTR_PROMPT.format(texto=normalize_text(texto.lower()))\n    tokens = tokenizer(inp, return_tensors=\"pt\", truncation=True, max_length=MAX_INPUT_LEN).to(model.device)\n    \n    with torch.no_grad():\n        out = model.generate(\n            **tokens,\n            max_new_tokens=max_new_tokens,\n            num_beams=num_beams,\n            length_penalty=0.9,\n            early_stopping=True,\n        )\n        \n    return tokenizer.decode(out[0], skip_special_tokens=True).strip()\n\nexemplo = 'INSTITUI O PROGRAMA “VOLTAR A ESTUDAR MUDA TUDO”, COM O OBJETIVO DE PROMOVER CAMPANHAS DE INCENTIVO À MATRÍCULA E VALORIZAÇÃO DA EDUCAÇÃO DE JOVENS E ADULTOS (EJA), NO ÂMBITO DO MUNICÍPIO DE NATAL/RN.'\n# GPT Output: Promover campanhas de incentivo à matrícula e valorização da Educação de Jovens e Adultos (EJA) no município de Natal.\nprint(predict(exemplo))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:51:31.259730Z","iopub.execute_input":"2025-11-14T22:51:31.260024Z","iopub.status.idle":"2025-11-14T22:51:33.641190Z","shell.execute_reply.started":"2025-11-14T22:51:31.260007Z","shell.execute_reply":"2025-11-14T22:51:33.640502Z"}},"outputs":[{"name":"stdout","text":"Criar o programa “voltar a estudar muda tudo” com campanhas de incentivo à matrícula e valorização da educação de jovens e adultos no município.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from peft import PeftModel\n\nfused_dir = OUTPUT_DIR + \"-merged\"\n\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16 if FP16 else torch.float32)\npeft_loaded = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n\nmerged = peft_loaded.merge_and_unload()\nmerged.save_pretrained(fused_dir)\ntokenizer.save_pretrained(fused_dir)\n\nprint(f\"Modelo fundido salvo em: {fused_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:53:14.112434Z","iopub.execute_input":"2025-11-14T22:53:14.113171Z","iopub.status.idle":"2025-11-14T22:53:16.524615Z","shell.execute_reply.started":"2025-11-14T22:53:14.113141Z","shell.execute_reply":"2025-11-14T22:53:16.523779Z"}},"outputs":[{"name":"stdout","text":"Modelo fundido salvo em: ./outputs_ptt5v2-merged\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:53:37.885156Z","iopub.execute_input":"2025-11-14T22:53:37.885454Z","iopub.status.idle":"2025-11-14T22:53:37.944031Z","shell.execute_reply.started":"2025-11-14T22:53:37.885433Z","shell.execute_reply":"2025-11-14T22:53:37.943458Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\napi = HfApi()\n\nMODEL_REPO = \"thiagoambiel/ptt5v2-pl-text2action\"\napi.create_repo(MODEL_REPO, repo_type=\"model\", private=True, exist_ok=True)\n\nMODEL_REPO","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:53:44.459046Z","iopub.execute_input":"2025-11-14T22:53:44.459830Z","iopub.status.idle":"2025-11-14T22:53:44.525287Z","shell.execute_reply.started":"2025-11-14T22:53:44.459772Z","shell.execute_reply":"2025-11-14T22:53:44.524681Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'thiagoambiel/ptt5v2-pl-text2action'"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from huggingface_hub import upload_folder\n\nupload_folder(\n    folder_path=\"/kaggle/working/outputs_ptt5v2-merged\",\n    repo_id=MODEL_REPO,\n    repo_type=\"model\",\n    commit_message=\"PTT5-v2 LoRA com BERTScore 0.84\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:54:37.427437Z","iopub.execute_input":"2025-11-14T22:54:37.427749Z","iopub.status.idle":"2025-11-14T22:54:46.265696Z","shell.execute_reply.started":"2025-11-14T22:54:37.427723Z","shell.execute_reply":"2025-11-14T22:54:46.265075Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b6e02e098a64501b4dce887a73275c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"153f5d1f71ff4876953fa7882dad3a76"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/thiagoambiel/ptt5v2-pl-text2action/commit/3e4a754d75fbe031d9b1fcc1ce894ffe61f5d66c', commit_message='PTT5-v2 LoRA com BERTScore 0.84', commit_description='', oid='3e4a754d75fbe031d9b1fcc1ce894ffe61f5d66c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/thiagoambiel/ptt5v2-pl-text2action', endpoint='https://huggingface.co', repo_type='model', repo_id='thiagoambiel/ptt5v2-pl-text2action'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":14}]}